{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS 5293 Assignment-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Welcome to CS 5293 (Text Analytics)!\n",
        "\n",
        "For this assignment 1, the main goal is to help you setup your python environment, and do some warmup coding for the following two parts:\n",
        "* Part 1. Analyze the vocabulary of large language model Llama-3.1 (40')\n",
        "* Part 2. Build your own N-gram model (60')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_u7lr90W2Ok"
      },
      "source": [
        "# Part 1. Analyze Llama-3.1 Vocabulary (40')\n",
        "\n",
        "In the lecture 2, we learned the Heap's law. As the corpus size grow, we don't hope the vocabulary continuously go larger, but we still the vocabulary cover all the words to avoice the out-of-vocabulary error(OOV). Hence, we learned the subword tokenization, typically the Byte Pair Encoding(BPE).\n",
        "\n",
        "From BERT, Roberta, T5, Llama-1, to Llama-2, majority of these language models vocabulary size is often around 30K. However, Llama-3 replaced the original tokenizer Sentence Piece [1] with the TikToken [2] used in OpenAI models, which has 128K vocabulary size. \n",
        "\n",
        "On the other side, larger vocabulary size also means more token efficient (i.e., fewer tokens are necessary to encode the same piece of text relative to Llama-2), which makes inference more efficient. However, the optimal size of vocabulary is still unknown. \n",
        "\n",
        "Here, we have a list of the \"tokens\" that are in the \"Llama-3.1-8B\" vocabulary file (../data/Llama-3.1-8b/vocab.txt), with one \"token\" per line. This goal of this assignment is to write a program to identify what are the 128K tokens in the Llama-3.1-8B vocabulary. \n",
        "\n",
        "In this jupyter notebook, we have conducted an initial investigation with simple regex via linux command to answer the simple question of \"How many punctunations/digits/special tokens are in there?\" \n",
        "For the remaning tokens in the vocabulary, please select only ONE research question about a special subset of tokens, (1) write a python program to obtain them, and (2) report the total number tokens in that selected subset. \n",
        "\n",
        "## Grading Criteria\n",
        "* (10') Be able to set up the envrionment, read and run through all the code via VSCode or your own jupternote, and understand how to use regex pattern to analyze the text. Hence, to get this score, you have to run each cell of the Part 1 to get an output to indicate that you could run that. \n",
        "* (30') Investigate your own research question.\n",
        "\n",
        "## Recommended open questions (not limited to this list, and not limited to English)\n",
        "* How many English roots are covered in the vocabulary? (../data/english.roots.list.build.json)\n",
        "* How many whole English words are there in the vocabulary? (../check_word.ipynb)\n",
        "* How emojis are tokenized in the tokenizer? What are the related tokens in the vocabulary?\n",
        "* How combining characters are tokenized in the tokenizer? What are the related tokens in the vocabulary?\n",
        "* If we define the bytes in each token as the token length? Could you figure out the longest token in the vocabulary? Please analyze.\n",
        "\n",
        "## Other Unhelpful Reading for This Assignment: \n",
        "1. Sentence Piece. https://github.com/google/sentencepiece\n",
        "2. Tiktoken. Think of the simple version of BPE algorithm we learned in lecture 2. The educational.py in Tiktoken algorithm offers an implementation of that.\n",
        " https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/_educational.py#L119\n",
        "3. Llama3.1 use a different `pat_str` to split the sentence, a bunch of special tokens, and larger vocabulary size: https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L21, please consider the special tokens as the shape of \"<|.*|>\"\n",
        "4. Here is an OpenAI tutorial to compare different vocabulary of their models from GPT2 to recent GPT-o1. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "5. Research on scaling laws of vocabulary also has been studied this. https://arxiv.org/abs/2407.13623. At the same time, researcher also studied the unfaireness of the tokenizer for different languages. https://arxiv.org/pdf/2305.15425 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup:  Python Environment and VSCode\n",
        "import the environment.yml under our root assignment folder, and create and activate your environment via the setup_tutorial.ipynb. \n",
        "\n",
        "```\n",
        "mamba env create -n cs5293-1 -f ../environment.yml\n",
        "mamba activate cs5293-1\n",
        "```\n",
        "\n",
        "if you failed to run the above command your in terminal. This is because the exported environmental.yml file by mamba is not cross-platform. It may fail in your OS envrionment. \n",
        "So you need to create your own \"cs5293-1\" from scratch. A simple rule to use mamba and pip, only use mamba for system-related packages, such as cuda, python or others. But for python libraries, e.g. transformers, torch, priotize to use `pip install` first, then `mamba install`. \n",
        "\n",
        "Please use the following commands:\n",
        "```\n",
        "mamba env create -n cs5293-1 python=3.10\n",
        "mamba activate cs5293-1\n",
        "pip install transformers torch\n",
        "```\n",
        "Hopefully all the above steps are sucessful, Good luck!\n",
        "Then you need to go the top-right corner of the VSCode, click the `Select Kernel`, (and `Select Another Kernel` if needed) to select the above cs5293-1 envrionment. \n",
        "\n",
        "If your VSCode didn't find the `cs5293-1` environment, please try to fully quit your VSCode and reopen the project folder, try the `Select Kernel` again. \n",
        "\n",
        "As mentioned in this webpage, starting from VS Code version 1.86.0, Microsoft now drop support for older operating system with glibc<2.28, which include ALL OSCER compute nodes. Until we upgrade our entire supercomputer to a newer operating system, your ONLY choice is to use *VS Code Desktop AND CLI version 1.85.2* via the links in this webpage https://www.ou.edu/oscer/support/VS_Code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Launch VSCode, and *Open the whole assignment-1 folder* in your VSCode.\n",
        "Then open this jupyter notebook Assignment-1.ipynb in VSCode by clicking the Explorer in the left side bar. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/sairishith/Downloads/Spring2025-cs5293-1-master/notebooks', '/Users/sairishith/miniforge3/envs/cs5293-1/lib/python310.zip', '/Users/sairishith/miniforge3/envs/cs5293-1/lib/python3.10', '/Users/sairishith/miniforge3/envs/cs5293-1/lib/python3.10/lib-dynload', '', '/Users/sairishith/miniforge3/envs/cs5293-1/lib/python3.10/site-packages', '/var/folders/s1/jq1jylpx1zq5fcqqyfyt0jg00000gn/T/tmpzpbsdhn2', '../src', '../src']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"../src\")\n",
        "print(sys.path)  \n",
        "#Disabling the parallelism of the tokenizers to avoid issues with the multiprocessing\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "#Importing the necessary modules for this assignment \n",
        "import vocab_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'../data/Llama-3.1-8b/vocab.txt'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DON'T USE THIS LLAMA2 MODEL FOR NOW\n",
        "#llama2_model_name = \"meta-llama/Llama-2-7b\"\n",
        "#llama2_local_model = \"../data/Llama-2-7b\"\n",
        "\n",
        "# If you understand the vocab_utils.py file, \n",
        "# you can freely change the following variables to test other models.\n",
        "# Otherwise, please DON'T change these variables.\n",
        "llama3_model_name = \"meta-llama/Llama-3.1-8b\"\n",
        "llama3_local_model = \"../data/Llama-3.1-8b\"\n",
        "\n",
        "vocab3_file = os.path.join(llama3_local_model,\"vocab.txt\")\n",
        "vocab3_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the tokenizer from the remote huggingface repo via the name of the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please ignore this cell, because it requires a license to do that. \n",
        "# The purpose of this is to load the tokenizer remotely and save it in your local folder. \n",
        "# You need to obtain a license for your own access to the model.\n",
        "# https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "# via the steps here. https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172\n",
        "# Feel free to do that because if you will need this in Assignment 4 or your project.\n",
        "# But you don't need that for this assignment. \n",
        "# We have saved the tokenizer for you locally and store them in the data folder.\n",
        "#tokenizer = vocab_utils.save_tokenizer_to_local(llama3_model_name, llama3_local_model)\n",
        "#vocab_utils.save_vocab(tokenizer, vocab3_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Llama3 Tokenizer Locally\n",
        "For this assignment, we will load the Llama3 tokenizer locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer loaded from ../data/Llama-3.1-8b\n"
          ]
        }
      ],
      "source": [
        "tokenizer = vocab_utils.load_local_tokenizer(llama3_local_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Llama3 Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test on English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please investigate the whitespace, line wrap and soon on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenized_sentences for \n",
            "BuyableInstoreAndOnline\n",
            "\n",
            "['Buy', 'able', 'In', 'store', 'And', 'Online', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "non_white_space_sequence = \"BuyableInstoreAndOnline\\n\"\n",
        "tokenized_sentences = tokenizer.tokenize(non_white_space_sequence)\n",
        "print(f\"tokenized_sentences for \\n{non_white_space_sequence}\\n{tokenized_sentences}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for token = Buy\n",
            "B, U+0042\n",
            "u, U+0075\n",
            "y, U+0079\n",
            "for token = able\n",
            "a, U+0061\n",
            "b, U+0062\n",
            "l, U+006c\n",
            "e, U+0065\n",
            "for token = ĠIn\n",
            "Ġ, U+0120\n",
            "I, U+0049\n",
            "n, U+006e\n",
            "for token = Ġstore\n",
            "Ġ, U+0120\n",
            "s, U+0073\n",
            "t, U+0074\n",
            "o, U+006f\n",
            "r, U+0072\n",
            "e, U+0065\n",
            "for token = ĠAnd\n",
            "Ġ, U+0120\n",
            "A, U+0041\n",
            "n, U+006e\n",
            "d, U+0064\n",
            "for token = ĠOnline\n",
            "Ġ, U+0120\n",
            "O, U+004f\n",
            "n, U+006e\n",
            "l, U+006c\n",
            "i, U+0069\n",
            "n, U+006e\n",
            "e, U+0065\n",
            "for token = Ċ\n",
            "Ċ, U+010a\n",
            "tokenized_sentences for \n",
            "Buyable In store And Online\n",
            "\n",
            "['Buy', 'able', 'ĠIn', 'Ġstore', 'ĠAnd', 'ĠOnline', 'Ċ']\n"
          ]
        }
      ],
      "source": [
        "white_space_sequence = \"Buyable In store And Online\\n\"\n",
        "tokenized_sentences = tokenizer.tokenize(white_space_sequence)\n",
        "# please pay attention to the special whitespace and \\n in the sequence.\n",
        "# they are just stored as unicode characters, not some newly added characters.\n",
        "# (0x20 is space, and they add 0x100 to every symbol they have to encode)\n",
        "# (0x0a is newline, and they add 0x100 to every symbol they have to encode) \n",
        "for token in tokenized_sentences:\n",
        "    print(f\"for token = {token}\")\n",
        "    for ch in token:\n",
        "        print(f\"{ch}, U+{ord(ch):04x}\")\n",
        "print(f\"tokenized_sentences for \\n{white_space_sequence}\\n{tokenized_sentences}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "😀, U+1f600\n",
            "🙅, U+1f645\n",
            "🏻, U+1f3fb\n",
            "tokenized_sentences for \n",
            "😀🙅🏻\n",
            "['ðŁĺ', 'Ģ', 'ðŁ', 'Ļ', 'ħ', 'ðŁ', 'ı', '»']\n"
          ]
        }
      ],
      "source": [
        "# testing emojis, https://en.wikipedia.org/wiki/Emoticons_(Unicode_block)\n",
        "sequence = \"😀🙅🏻\" # The second emoji is so called emoji with a modifer.\n",
        "# The string is naturally in unicode in Python.\n",
        "# You could print the unicode for each character in the string.\n",
        "for ch in sequence:\n",
        "    print(f\"{ch}, U+{ord(ch):04x}\")\n",
        "tokenized_sentences = tokenizer.tokenize(sequence)\n",
        "print(f\"tokenized_sentences for \\n{sequence}\\n{tokenized_sentences}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test on Combining Characer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T 84\n",
            "̵ 821\n",
            "o 111\n",
            "̵ 821\n",
            "k 107\n",
            "̴ 820\n",
            "e 101\n",
            "̵ 821\n",
            "n 110\n",
            "̷ 823\n",
            "i 105\n",
            "̸ 824\n",
            "z 122\n",
            "̵ 821\n",
            "a 97\n",
            "̵ 821\n",
            "t 116\n",
            "̴ 820\n",
            "i 105\n",
            "̴ 820\n",
            "o 111\n",
            "̴ 820\n",
            "n 110\n",
            "̵ 821\n",
            "  32\n",
            "̴ 820\n",
            "i 105\n",
            "̶ 822\n",
            "s 115\n",
            "̵ 821\n",
            "  32\n",
            "̷ 823\n",
            "f 102\n",
            "̸ 824\n",
            "u 117\n",
            "̴ 820\n",
            "n 110\n",
            "̶ 822\n",
            "! 33\n",
            "̵ 821\n",
            "tokenized_sentences for \n",
            "T̵o̵k̴e̵n̷i̸z̵a̵t̴i̴o̴n̵ ̴i̶s̵ ̷f̸u̴n̶!̵\n",
            "['T', 'Ì', 'µ', 'o', 'Ì', 'µ', 'k', 'Ì', '´', 'e', 'Ì', 'µ', 'n', 'Ì', '·', 'i', 'Ì', '¸', 'z', 'Ì', 'µ', 'a', 'Ì', 'µ', 't', 'Ì', '´', 'i', 'Ì', '´', 'o', 'Ì', '´', 'n', 'Ì', 'µ', 'Ġ', 'Ì', '´', 'i', 'Ì', '¶', 's', 'Ì', 'µ', 'Ġ', 'Ì', '·', 'f', 'Ì', '¸', 'u', 'Ì', '´', 'n', 'Ì', '¶', '!', 'Ì', 'µ']\n"
          ]
        }
      ],
      "source": [
        "# The combination characters for 'Tokenization is fun!'\n",
        "# You could generate more here https://lingojam.com/ZalgoText \n",
        "sequence = \"T̵o̵k̴e̵n̷i̸z̵a̵t̴i̴o̴n̵ ̴i̶s̵ ̷f̸u̴n̶!̵\" \n",
        "# The string is naturally in unicode in Python.\n",
        "# You could print the unicode for each character in the string.\n",
        "for ch in sequence:\n",
        "    print(ch, ord(ch))\n",
        "tokenized_sentences = tokenizer.tokenize(sequence)\n",
        "print(f\"tokenized_sentences for \\n{sequence}\\n{tokenized_sentences}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vupo1DqW2Ol"
      },
      "source": [
        "### Initial Exploration for Llama3's Vocabulary\n",
        "\n",
        "As an exmaple, in the following, we show a list of linux commands for an initial analysis on the vacabulary. Please to find the corresponding python code to get the same function, which is not hard. These are just an optional practice for verification of your python code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following command will count in total how many lines in the vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "7_LZfwicW2Ol"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  128256 ../data/Llama-3.1-8b/vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l ../data/Llama-3.1-8b/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYzupR_W2Ol"
      },
      "source": [
        "So we're in the right ballpark with 128256 lines (tokens). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "line_count = 128256\n"
          ]
        }
      ],
      "source": [
        "# Python code for count lines in the file ../data/Llama-3.1-8b/vocab.txt\n",
        "line_count = 0\n",
        "with open(vocab3_file) as f:\n",
        "    line_count = sum(1 for _ in f)\n",
        "\n",
        "print(f\"line_count = {line_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the above python code will read the vocabulary file, and count each lines(one token per line).\n",
        "Let's see what's in there.\n",
        "Because the list is too long. You could use the command `head` to print the first 10 lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "T-cCFc5-W2Ol"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\n",
            "\"\n",
            "#\n",
            "$\n",
            "%\n",
            "&\n",
            "'\n",
            "(\n",
            ")\n",
            "*\n"
          ]
        }
      ],
      "source": [
        "!head ../data/Llama-3.1-8b/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_KErIyeW2Ol"
      },
      "source": [
        "Ok, those look like the initial set of characters we mentioned in our lectures. Remember we said that subword tokenization algorithms start with an initial vocabulary of characters. In the lecture 2, we only consider the letters and numbers.  That's really not quite right, if you're using arbitrary web docs and things like Wikipedia then you're going to run into a lot of odd characters, such as the combination characters used in Fentayln example.  Better to just use all the unicode characters that occur in the training text. Let's see what we get we look at all the single character entries in the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2sJcxBI1W2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\n",
            "\"\n",
            "#\n",
            "$\n",
            "%\n",
            "&\n",
            "'\n",
            "(\n",
            ")\n",
            "*\n"
          ]
        }
      ],
      "source": [
        "!grep '^.$' ../data/Llama-3.1-8b/vocab.txt  | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "frMUWQUyW2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With grep command, there are      256 single characters, which are all the 2^8 bytes\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "single_char_num=$(grep '^.$' ../data/Llama-3.1-8b/vocab.txt  | wc -l)\n",
        "echo \"With grep command, there are ${single_char_num} single characters, which are all the 2^8 bytes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With python regex, there are 256 single characters, which are all the 2^8 bytes\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# please learn to use python regex. https://www.w3schools.com/python/python_regex.asp\n",
        "single_char_num = 0\n",
        "with open(vocab3_file) as f:\n",
        "    for token in f:\n",
        "        if re.match(r'^.$', token):\n",
        "            single_char_num += 1\n",
        "print(f\"With python regex, there are {single_char_num} single characters, which are all the 2^8 bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbLaYAbwW2Om"
      },
      "source": [
        "Next, let us take a look at the special tokens as the shape of \"<|.*|>\". https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L21 \n",
        "Please try to write a python program to replicate this result for detecting the special tokens. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>\n",
            "<|end_of_text|>\n",
            "<|reserved_special_token_0|>\n",
            "<|reserved_special_token_1|>\n",
            "<|finetune_right_pad_id|>\n",
            "<|reserved_special_token_2|>\n",
            "<|start_header_id|>\n",
            "<|end_header_id|>\n",
            "<|eom_id|>\n",
            "<|eot_id|>\n",
            "<|python_tag|>\n",
            "<|reserved_special_token_3|>\n",
            "<|reserved_special_token_4|>\n",
            "<|reserved_special_token_5|>\n",
            "<|reserved_special_token_6|>\n",
            "<|reserved_special_token_7|>\n",
            "<|reserved_special_token_8|>\n",
            "<|reserved_special_token_9|>\n",
            "<|reserved_special_token_10|>\n",
            "<|reserved_special_token_11|>\n",
            "<|reserved_special_token_12|>\n",
            "<|reserved_special_token_13|>\n",
            "<|reserved_special_token_14|>\n",
            "<|reserved_special_token_15|>\n",
            "<|reserved_special_token_16|>\n",
            "<|reserved_special_token_17|>\n",
            "<|reserved_special_token_18|>\n",
            "<|reserved_special_token_19|>\n",
            "<|reserved_special_token_20|>\n",
            "<|reserved_special_token_21|>\n",
            "<|reserved_special_token_22|>\n",
            "<|reserved_special_token_23|>\n",
            "<|reserved_special_token_24|>\n",
            "<|reserved_special_token_25|>\n",
            "<|reserved_special_token_26|>\n",
            "<|reserved_special_token_27|>\n",
            "<|reserved_special_token_28|>\n",
            "<|reserved_special_token_29|>\n",
            "<|reserved_special_token_30|>\n",
            "<|reserved_special_token_31|>\n",
            "<|reserved_special_token_32|>\n",
            "<|reserved_special_token_33|>\n",
            "<|reserved_special_token_34|>\n",
            "<|reserved_special_token_35|>\n",
            "<|reserved_special_token_36|>\n",
            "<|reserved_special_token_37|>\n",
            "<|reserved_special_token_38|>\n",
            "<|reserved_special_token_39|>\n",
            "<|reserved_special_token_40|>\n",
            "<|reserved_special_token_41|>\n"
          ]
        }
      ],
      "source": [
        "!grep -E -o '^<\\|.*\\|>$' ../data/Llama-3.1-8b/vocab.txt| head -50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     256\n"
          ]
        }
      ],
      "source": [
        "!grep -E -o '^<\\|.*\\|>$' ../data/Llama-3.1-8b/vocab.txt|wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So there are 256 used or reserved special tokens in Llama-3.1-8b vocabulary. Next we found the special Ġ is used for the leading whitespace of the token. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2UeYfc25W2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   57875\n"
          ]
        }
      ],
      "source": [
        "!grep '^Ġ'< ../data/Llama-3.1-8b/vocab.txt | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "vR_gMwnPW2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "00\n",
            "20\n",
            "10\n",
            "201\n",
            "12\n",
            "19\n",
            "11\n",
            "32\n",
            "16\n",
            "15\n",
            "25\n",
            "000\n",
            "30\n",
            "18\n",
            "14\n",
            "13\n",
            "100\n",
            "200\n",
            "17\n",
            "50\n",
            "24\n",
            "64\n",
            "40\n",
            "22\n",
            "60\n",
            "23\n",
            "99\n",
            "80\n",
            "27\n",
            "28\n",
            "26\n",
            "33\n",
            "29\n",
            "21\n",
            "01\n",
            "35\n",
            "45\n",
            "37\n",
            "36\n",
            "90\n",
            "34\n",
            "38\n",
            "70\n",
            "75\n",
            "44\n",
            "55\n",
            "39\n",
            "31\n",
            "48\n",
            "66\n",
            "05\n",
            "08\n",
            "202\n",
            "04\n",
            "65\n",
            "88\n",
            "02\n",
            "49\n",
            "78\n",
            "09\n",
            "199\n",
            "07\n",
            "68\n",
            "47\n",
            "500\n",
            "06\n",
            "95\n",
            "46\n",
            "77\n",
            "03\n",
            "59\n",
            "58\n",
            "42\n",
            "69\n",
            "67\n",
            "300\n",
            "41\n",
            "255\n",
            "57\n",
            "98\n",
            "43\n",
            "400\n",
            "56\n",
            "97\n",
            "198\n",
            "150\n",
            "51\n",
            "87\n",
            "52\n",
            "001\n",
            "256\n",
            "96\n",
            "86\n",
            "102\n",
            "53\n",
            "120\n",
            "54\n",
            "128\n",
            "197\n",
            "123\n",
            "89\n",
            "79\n",
            "101\n",
            "800\n",
            "76\n",
            "111\n",
            "600\n",
            "110\n",
            "250\n",
            "196\n",
            "180\n",
            "85\n",
            "72\n",
            "63\n",
            "999\n",
            "62\n",
            "61\n",
            "74\n",
            "84\n",
            "130\n",
            "91\n",
            "192\n",
            "81\n",
            "73\n",
            "71\n",
            "83\n",
            "92\n",
            "82\n",
            "003\n",
            "195\n",
            "94\n",
            "160\n",
            "93\n",
            "194\n",
            "125\n",
            "105\n",
            "108\n",
            "002\n",
            "127\n",
            "360\n",
            "104\n",
            "140\n",
            "103\n",
            "700\n",
            "190\n",
            "112\n",
            "193\n",
            "115\n",
            "106\n",
            "900\n",
            "404\n",
            "191\n",
            "107\n",
            "109\n",
            "010\n",
            "204\n",
            "121\n",
            "114\n",
            "116\n",
            "113\n",
            "170\n",
            "122\n",
            "240\n",
            "512\n",
            "005\n",
            "117\n",
            "220\n",
            "350\n",
            "004\n",
            "333\n",
            "210\n",
            "124\n",
            "135\n",
            "118\n",
            "144\n",
            "168\n",
            "119\n",
            "131\n",
            "141\n",
            "188\n",
            "189\n",
            "126\n",
            "132\n",
            "133\n",
            "134\n",
            "320\n",
            "145\n",
            "203\n",
            "187\n",
            "151\n"
          ]
        }
      ],
      "source": [
        "!grep -E -o '[[:digit:]]+' < ../data/Llama-3.1-8b/vocab.txt| head -200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htO2H-fFW2Om"
      },
      "source": [
        "Let's see how many tokens are started with white space character in the unicode 'Ġ'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ZMVsmnC-W2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   57875\n"
          ]
        }
      ],
      "source": [
        "!grep '^Ġ' ../data/Llama-3.1-8b/vocab.txt |  wc -l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "R2aC2HrwW2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in\n",
            "er\n",
            "on\n",
            "re\n",
            "at\n",
            "st\n",
            "en\n",
            "or\n",
            "ĊĊ\n",
            "le\n",
            "it\n",
            "an\n",
            "ar\n",
            "al\n",
            ";Ċ\n",
            "ou\n",
            "is\n",
            "ing\n",
            "es\n",
            "ion\n",
            "ed\n",
            "ic\n",
            "et\n",
            "ĉĉ\n",
            "ro\n",
            "as\n",
            "el\n",
            "ct\n",
            "nd\n",
            "ent\n",
            "id\n",
            "am\n",
            "--\n",
            "om\n",
            ");Ċ\n",
            "im\n",
            "čĊ\n",
            "il\n",
            "//\n",
            "ur\n",
            "se\n",
            "ex\n",
            "ad\n",
            "ch\n",
            "ut\n",
            "if\n",
            "**\n",
            "em\n",
            "ol\n",
            "th\n",
            ")Ċ\n",
            "ig\n",
            "iv\n",
            ",Ċ\n",
            "ce\n",
            "od\n",
            "ate\n",
            "ag\n",
            "ay\n",
            "ot\n",
            "us\n",
            "un\n",
            "ul\n",
            "ue\n",
            "ow\n",
            "ew\n",
            "ation\n",
            "()\n",
            "ab\n",
            "ort\n",
            "um\n",
            "ame\n",
            "pe\n",
            "tr\n",
            "ck\n",
            "âĢ\n",
            "ist\n",
            "----\n",
            ".ĊĊ\n",
            "he\n",
            "lo\n",
            "ers\n",
            "ap\n",
            "ub\n",
            "ass\n",
            "int\n",
            ">Ċ\n",
            "ly\n",
            "urn\n",
            ";ĊĊ\n",
            "av\n",
            "port\n",
            "ir\n",
            "->\n",
            "nt\n",
            "ction\n",
            "end\n",
            "00\n",
            "ith\n",
            "out\n",
            "turn\n",
            "our\n",
            "lic\n",
            "res\n",
            "pt\n",
            "==\n",
            "ver\n",
            "age\n",
            "ht\n",
            "ext\n",
            "=\"\n",
            "****\n",
            "ess\n",
            "os\n",
            "and\n",
            "ect\n",
            "ke\n",
            "rom\n",
            "con\n",
            "(\"\n",
            "qu\n",
            "lass\n",
            "iz\n",
            "de\n",
            "op\n",
            "up\n",
            "get\n",
            "ile\n",
            "ata\n",
            "ore\n",
            "ri\n",
            ";čĊ\n",
            "ĉĉĉĉ\n",
            "ter\n",
            "ain\n",
            "art\n",
            "ack\n",
            "import\n",
            "ublic\n",
            "est\n",
            "ment\n",
            "able\n",
            "ine\n",
            "ill\n",
            "ind\n",
            "ere\n",
            "::\n",
            "ity\n",
            "elf\n",
            "ight\n",
            "('\n",
            "orm\n",
            "ult\n",
            "str\n",
            "..\n",
            "\",\n",
            "ype\n",
            "pl\n",
            "20\n",
            "ld\n",
            "oc\n",
            ":Ċ\n",
            "--------\n",
            ".s\n",
            "{Ċ\n",
            "',\n",
            "ant\n",
            "ase\n",
            ".c\n",
            "</\n",
            "ave\n",
            "ang\n",
            "âĢĻ\n",
            "_t\n",
            "ert\n",
            "ial\n",
            "act\n",
            "}Ċ\n",
            "ive\n",
            "ode\n",
            "ost\n",
            "og\n",
            "ord\n",
            "alue\n",
            "all\n",
            "ff\n",
            "();Ċ\n",
            "ont\n",
            "ime\n",
            "are\n",
            "ies\n",
            "ize\n",
            "ure\n",
            "ire\n",
            ".p\n",
            "ice\n",
            "ast\n",
            "ption\n",
            "tring\n",
            "ok\n",
            "grep: stdout: Broken pipe\n"
          ]
        }
      ],
      "source": [
        "!grep -v '\\[' ../data/Llama-3.1-8b/vocab.txt | grep -v '^.$' | grep -v '^Ġ' | head -200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "705DERRCW2Om"
      },
      "source": [
        "Although its not stated, this is obviously a frequency ordered list. \"in\" is at the top. Some of these are recognizable as English suffixes (-ed, -ing, -ly, etc).\n",
        "\n",
        "Let's just sort it alphanumerically to see what's in there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "GdbdHGsXW2Om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!\n",
            "!!!\n",
            "!!!!\n",
            "!!!!!\n",
            "!!!!!!!!\n",
            "!!!!ĊĊ\n",
            "!!!Ċ\n",
            "!!!ĊĊ\n",
            "!!\");Ċ\n",
            "!!)Ċ\n",
            "!!,\n",
            "!!.\n",
            "!!Ċ\n",
            "!!ĊĊ\n",
            "!\"\n",
            "!\")\n",
            "!\");\n",
            "!\");Ċ\n",
            "!\");ĊĊ\n",
            "!\");čĊ\n",
            "!\")Ċ\n",
            "!\")ĊĊ\n",
            "!\")čĊ\n",
            "!\",\n",
            "!\",Ċ\n",
            "!\".\n",
            "!\";Ċ\n",
            "!\";čĊ\n",
            "!\"Ċ\n",
            "!\"ĊĊ\n",
            "!'\n",
            "!');Ċ\n",
            "!')Ċ\n",
            "!')ĊĊ\n",
            "!',\n",
            "!',Ċ\n",
            "!';Ċ\n",
            "!'Ċ\n",
            "!(\n",
            "!(\"\n",
            "!(\"{\n",
            "!(\"{}\",\n",
            "!(:\n",
            "!(Ċ\n",
            "!)\n",
            "!),\n",
            "!).\n",
            "!).ĊĊ\n",
            "!)Ċ\n",
            "!)ĊĊ\n",
            "!*\n",
            "!*\\Ċ\n",
            "!,\n",
            "!,Ċ\n",
            "!--\n",
            "!.\n",
            "!..\n",
            "!.ĊĊ\n",
            "!/\n",
            "!:\n",
            "!;Ċ\n",
            "!<\n",
            "!</\n",
            "!=\n",
            "!=\"\n",
            "!='\n",
            "!=(\n",
            "!=-\n",
            "!==\n",
            "!?\n",
            "!I\n",
            "!\\\n",
            "!]\n",
            "!important\n",
            "!Â»\n",
            "!âĢĻ\n",
            "!âĢľ\n",
            "!âĢľĊĊ\n",
            "!âĢĿ\n",
            "!âĢĿĊĊ\n",
            "!Ċ\n",
            "!ĊĊ\n",
            "!ĊĊĊ\n",
            "!ĊĊĊĊ\n",
            "!ĊĊĊĊĊĊ\n",
            "!čĊ\n",
            "\"\"\n",
            "\"\"\"\n",
            "\"\"\"),Ċ\n",
            "\"\"\")Ċ\n",
            "\"\"\",Ċ\n",
            "\"\"\".\n",
            "\"\"\"Ċ\n",
            "\"\"\"ĊĊ\n",
            "\"\"\"ĊĊĊ\n",
            "\"\"\"čĊ\n",
            "\"\"\"čĊčĊ\n",
            "\"\",\n",
            "\"\",Ċ\n",
            "\"\".\n",
            "\"\":\n",
            "\"\"Ċ\n",
            "\"#\n",
            "\"$\n",
            "\"${\n",
            "\"%\n",
            "\"%(\n",
            "\"&\n",
            "\"'\n",
            "\"',\n",
            "\"',Ċ\n",
            "\"';\n",
            "\"';Ċ\n",
            "\"'Ċ\n",
            "\"(\n",
            "\")\n",
            "\")!=\n",
            "\")(\n",
            "\"))\n",
            "\")))\n",
            "\"))))Ċ\n",
            "\")));\n",
            "\")));Ċ\n",
            "\")));ĊĊ\n",
            "\")));čĊ\n",
            "\")))Ċ\n",
            "\")),\n",
            "\")),Ċ\n",
            "\")).\n",
            "\"));\n",
            "\"));Ċ\n",
            "\"));ĊĊ\n",
            "\"));čĊ\n",
            "\"));čĊčĊ\n",
            "\")){Ċ\n",
            "\")){čĊ\n",
            "\"))Ċ\n",
            "\"))ĊĊ\n",
            "\"))čĊ\n",
            "\")+\n",
            "\"),\n",
            "\"),\"\n",
            "\"),Ċ\n",
            "\"),ĊĊ\n",
            "\"),čĊ\n",
            "\")->\n",
            "\").\n",
            "\").Ċ\n",
            "\").ĊĊ\n",
            "\"):\n",
            "\"):Ċ\n",
            "\"):čĊ\n",
            "\");\n",
            "\");//\n",
            "\");}Ċ\n",
            "\");Ċ\n",
            "\");ĊĊ\n",
            "\");ĊĊĊ\n",
            "\");čĊ\n",
            "\");čĊčĊ\n",
            "\")==\n",
            "\")]\n",
            "\")]Ċ\n",
            "\")]ĊĊ\n",
            "\")]čĊ\n",
            "\"){\n",
            "\"){Ċ\n",
            "\"){čĊ\n",
            "\")}\n",
            "\")},Ċ\n",
            "\")}Ċ\n",
            "\")Ċ\n",
            "\")ĊĊ\n",
            "\")ĊĊĊ\n",
            "\")čĊ\n",
            "\")čĊčĊ\n",
            "\"*\n",
            "\"+\n",
            "\"+\"\n",
            "\"+Ċ\n",
            "\",\n",
            "\",\"\n",
            "\",\"\");Ċ\n",
            "\",\"\",\n",
            "\",\"\",\"\n",
            "\",\"#\n",
            "\",\"+\n",
            "\",\"\\\n",
            "\",$\n",
            "\",&\n",
            "\",'\n",
            "\",(\n",
            "\",-\n",
            "\",@\"\n",
            "\",__\n",
            "\",{\n",
            "\",Ċ\n",
            "\",ĊĊ\n",
            "\",čĊ\n",
            "\"-\n",
            "sort: Broken pipe\n"
          ]
        }
      ],
      "source": [
        "!grep -v '\\[' < ../data/Llama-3.1-8b/vocab.txt | grep -v '^.$' | grep -v '^Ġ'  | sort | head -200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hmm.  A lot of puntuations, and many with line wrap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   69647\n"
          ]
        }
      ],
      "source": [
        "!grep -v '\\[' < ../data/Llama-3.1-8b/vocab.txt | grep -v '^.$' | grep -v '^Ġ' | grep -v '^Ċ' | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epZW4pPAW2On"
      },
      "source": [
        "Let's just get the numbers that constitute the whole line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pHxaGPDOW2On"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    1358\n"
          ]
        }
      ],
      "source": [
        "!grep -E -o '[[:digit:]]+' ../data/Llama-3.1-8b/vocab.txt | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Submission for Part 1\n",
        "\n",
        "* (1) You need to make sure this jupyter note book having ran through all the cells with the outputs.(include the above cells and the new cells you will add below), and export a pdf file for your main submission.\n",
        "* (2) You should submit the whole Assignment-1 folder as a zip file. Your code and instruction should either use this jupyter notebook or put some seperate python file in the src folder. Any external resource should be placed in the data folder. We will use this to rerun your code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Put Your Research Question Here\n",
        "### How emojis are tokenized in the tokenizer? What are the related tokens in the vocabulary?\n",
        "### Why This Question?\n",
        "#### Emojis are a key part of modern communication, and understanding how they are tokenized is crucial for tasks like sentiment analysis, social media text processing, and chatbot interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```Please add a paragraph here to explain what is your research question?```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why This Question?\n",
        "Emojis are a key part of modern communication, and understanding how they are tokenized is crucial for tasks like sentiment analysis, social media text processing, and chatbot interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_file = \"../data/Llama-3.1-8b/vocab.txt\"\n",
        "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab_tokens = [line.strip() for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import emoji\n",
        "\n",
        "# Function to check if a token contains an emoji\n",
        "def is_emoji(token):\n",
        "    return bool(emoji.emoji_list(token))\n",
        "\n",
        "# Filter tokens that contain emojis\n",
        "emoji_tokens = [token for token in vocab_tokens if is_emoji(token)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emoji: 😀 → Tokens: ['ðŁĺ', 'Ģ']\n",
            "Emoji: 🙅🏻 → Tokens: ['ðŁ', 'Ļ', 'ħ', 'ðŁ', 'ı', '»']\n",
            "Emoji: 👨‍👩‍👧‍👦 → Tokens: ['ðŁ', 'ĳ', '¨', 'âĢį', 'ðŁ', 'ĳ', '©', 'âĢį', 'ðŁ', 'ĳ', '§', 'âĢį', 'ðŁ', 'ĳ', '¦']\n"
          ]
        }
      ],
      "source": [
        "sample_emojis = [\"😀\", \"🙅🏻\", \"👨‍👩‍👧‍👦\"]  # Simple emoji, emoji with modifier, and family emoji\n",
        "\n",
        "for emoji_char in sample_emojis:\n",
        "    tokenized = tokenizer.tokenize(emoji_char)\n",
        "    print(f\"Emoji: {emoji_char} → Tokens: {tokenized}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of emoji-related tokens in the vocabulary: 1641\n",
            "Sample emoji-related tokens:\n",
            "['©', '®', 'Ã©', 'ĠÃ©', 'Ã©s', 'ĠÂ©', 'ĠdÃ©', 'Ã©e', 'å®', 'ĠrÃ©']\n"
          ]
        }
      ],
      "source": [
        "num_emoji_tokens = len(emoji_tokens)\n",
        "print(f\"Number of emoji-related tokens in the vocabulary: {num_emoji_tokens}\")\n",
        "print(\"Sample emoji-related tokens:\")\n",
        "print(emoji_tokens[:10])  # Print the first 10 emoji-related tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPT-2 (Byte-Level BPE) Tokenization:\n",
            "Original Text: 😀🙅🏻🔥\n",
            "Tokenized Output: ['ðŁĺ', 'Ģ', 'ðŁ', 'Ļ', 'ħ', 'ðŁ', 'ı', '»', 'ðŁ', 'Ķ', '¥']\n",
            "Token IDs: [47249, 222, 8582, 247, 227, 8582, 237, 119, 8582, 242, 98]\n",
            "--------------------------------------------------\n",
            "\n",
            "BERT (WordPiece) Tokenization:\n",
            "Original Text: 😀🙅🏻🔥\n",
            "Tokenized Output: ['[UNK]']\n",
            "Token IDs: [100]\n",
            "--------------------------------------------------\n",
            "\n",
            "T5 (SentencePiece) Tokenization:\n",
            "Original Text: 😀🙅🏻🔥\n",
            "Tokenized Output: ['▁', '😀🙅🏻🔥']\n",
            "Token IDs: [3, 2]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the emoji sequence\n",
        "text = \"😀🙅🏻🔥\"\n",
        "\n",
        "# Tokenizer models to test\n",
        "models = {\n",
        "    \"GPT-2 (Byte-Level BPE)\": \"gpt2\",\n",
        "    \"BERT (WordPiece)\": \"bert-base-uncased\",\n",
        "    \"T5 (SentencePiece)\": \"t5-small\"\n",
        "}\n",
        "\n",
        "# Process each tokenizer\n",
        "for model_name, model_path in models.items():\n",
        "    print(f\"\\n{model_name} Tokenization:\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    \n",
        "    # Tokenize emojis\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"Original Text: {text}\")\n",
        "    print(f\"Tokenized Output: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion:\n",
        "\n",
        "Emoji tokenization depends on the tokenizer type:\n",
        "GPT-2: Splits emojis into byte-based tokens because it uses Byte-Level BPE.\n",
        "BERT: Replaces unseen emojis with [UNK] because WordPiece does not handle them well.\n",
        "T5: Handles some emojis as full tokens, but rare ones may be split.\n",
        "\n",
        "\n",
        "GPT-2 (Byte-Level BPE): Emojis are split into multiple byte tokens because GPT-2 operates at the byte level. Example: \"😀🙅🏻\" → ['ðŁĺ', 'Ģ', 'ðŁ', 'Ļ', 'ħ', 'ðŁ', 'ı', '»'].\n",
        "BERT (WordPiece): If an emoji was not in training data, it gets replaced with [UNK], meaning it is not tokenized properly.\n",
        "T5/mT5 (SentencePiece): Handles some emojis as full tokens but may split rare ones into subwords. Example: \"😀🙅🏻\" → ['▁😀', '▁🙅', '▁🏻']\n",
        "\n",
        "Related Tokens in the Vocabulary\n",
        "    In GPT-2, emoji tokens appear as byte sequences ('ðŁĺ', 'ðŁı', etc.), meaning emojis do not exist as single tokens.\n",
        "    In BERT, they may be missing ([UNK]).\n",
        "    In T5, they are either full tokens or split subwords, depending on frequency in training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Put All Your Code Cells Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Then please add more cells to study your questions as the above \n",
        "# You could write all your code in the cells below.\n",
        "# Or your code could be written in a python file in the src folder, then call it in the cells bellow.\n",
        "# Take the vocab_utils.py and the hello_world.py as an example, they are written in the src folder as single files. \n",
        "# But they are imported in the juptyer notebook and run in the cells. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2. N-Gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your task for this Part 2 is to build an N-gram language model from scratch. Your language modeling\n",
        "program should accept three input files: (1) a training corpus file, (2) a test sentences file, and (3) a seeds file, which will contain a list of words to begin the language generation process. \n",
        "In Part-1, you have learned to use jupyter notebooks. In this Part 2, you will learn the way of using command-line to run your program. Your program should accept three files as command-line arguments in the following order:\n",
        "\n",
        "                        ```python ngram.py <training file> <test file> <seeds_file>```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grading Criteria\n",
        "\n",
        "We will run your program on the files that we give you as well as new files to evaluate the\n",
        "generality and correctness of your code. So please test your program thoroughly! Even\n",
        "if your program works perfectly on the examples that we give you, that does not guarantee\n",
        "that it will work perfectly on diﬀerent test cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input Files\n",
        "\n",
        "All the input files located in the data folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training File\n",
        "The training file will consist of sentences, one sentence per line. For example, a training file might look like this:\n",
        "```\n",
        "I love natural language processing .\n",
        "This assignment looks like fun !\n",
        "```\n",
        "You should divide each sentence into unigrams based **solely on white space**. Note that this\n",
        "can produce isolated punctuation marks (when white space separates a punctuation mark\n",
        "from adjacent words) as well as words with punctuation symbols that are still attached (when\n",
        "white space does NOT separate a punctuation mark from an adjacent word). For example,\n",
        "consider the following sentence:\n",
        "```\n",
        "“This is a funny-looking sentence” , she said !\n",
        "```\n",
        "This sentence should be divided into exactly nine unigrams:\n",
        "(1) “This (2) is (3) a (4) funny-looking (5) sentence” (6) , (7) she (8) said (9) !\n",
        "\n",
        "### Test File\n",
        "The test file will have exactly the same format as the training file and it should be divided\n",
        "into unigrams exactly the same way. So please don't feel comfortable the punctuations, just use the white space to tokenize each raw sentence, not any processing. People may ask what about the validatio or develop set. Since the N-gram training is almost deterministic, so in this assignment, we will not use a develop set to select the model hyperparameters or a held-out set for further testing the generalization.\n",
        "\n",
        "### Seeds File\n",
        "The seeds file will have one word per line, and each word should be used to start the language generation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the N-gram Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create the N-gram language models, you will need to generate tables of frequency counts\n",
        "from the training corpus for unigrams (1-grams) and bigrams (2-grams). An N-gram should\n",
        "not cross sentence boundaries. All of your N-gram tables should be case-insensitive (i.e.,\n",
        "“the”, “The”, and “THE” should be treated as the same word).\n",
        "\n",
        "You should create three diﬀerent types of language models:\n",
        "* (a) A unigram language model with no smoothing.\n",
        "* (b) A bigram language model with no smoothing.\n",
        "* (c) A bigram language model with add-one smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can assume that the set of unigrams found in the training corpus is the entire universe\n",
        "of unigrams. We will not give you test sentences that contain unseen unigrams. So the\n",
        "vocabulary $V$ for this assignment is the set of unique unigrams that occur in the training\n",
        "corpus.\n",
        "\n",
        "However, we will give you test sentences that contain bigrams that did not appear in the\n",
        "training corpus. The n-grams will consist entirely of unigrams that appeared in the training\n",
        "corpus, but there may be new (previously unseen) combinations of the unigrams. The first\n",
        "two language models (a and b) do not use smoothing, so unseen bigrams should be assigned\n",
        "a probability of zero. For the last language model (c), you should use add-one smoothing to\n",
        "compute the probabilities for all of the bigrams.\n",
        "\n",
        "For bigrams, you will need to have a special pseudo-word “\\<s\\>” as a beginning-of-sentence symbol. Bigrams of the form \"\\<s\\>$w_i$\" mean that word $w_i$ occurs at the beginning of the sentence. Do NOT include \"\\<s\\>\" as a word in your vocabulary for the unigram language model or include \"\\<s\\>\" in the sentence probability for the unigram model.\n",
        "For simplicity, just use the unigram frequency count of $w_{k−1}$ to compute the conditional probability $P (w_k | w_{k−1})$. (This means you won’t have to worry about cases where w_{k−1} occurs at the end of the sentence and isn’t followed by anything.) For example, just compute $P (w_k | w_{k−1}) = count (w_{k−1}w_k) /count (w_{k−1})$.\n",
        "You should NOT use an end-of-sentence symbol. The last bigram for a sentence of length should\n",
        "represent the last 2 words of the sentence: $w_{n−1}w_n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### N-gram LM Task 1: Computing Sentence Probability (30')\n",
        "\n",
        "For each of the language models, you should create a function that computes the probability\n",
        "of a sentence $P(w_1 ... w_n)$ using that language model. Since the probabilities will get very\n",
        "small, you must do the probability computations in log space (as discussed in class, also see\n",
        "the lecture slides). Please do these calculations *using log base 2*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Output Specifications for Task 1\n",
        "\n",
        "Your program should print the following information for each test sentence. When printing\n",
        "the logprob numbers, please only print 4 digits after the decimal point. For example, print\n",
        "-8.9753864210 as -8.9754. The programming language will have a mechanism for controlling\n",
        "the number of digits that are printed. If $P(S) = 0$, then the logarithm is not defined, so\n",
        "print logprob(S) = undefined.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please print the following information, a sentence line, an empty line, and three probabilities formatted like this:\n",
        "\n",
        "```\n",
        "S = <sentence>\n",
        "\n",
        "Unigrams: logprob(S) = #\n",
        "Bigrams: logprob(S) = #\n",
        "Smoothed Bigrams: logprob(S) = #\n",
        "```\n",
        "For example, your output might look like this (the examples below are not real, they are\n",
        "just for illustration!):\n",
        "```\n",
        "S = Trump has given his second inaugural speech .\n",
        "\n",
        "Unigrams: logprob(S) = -6.5712\n",
        "Bigrams: logprob(S) = -9.2253\n",
        "Smoothed Bigrams: logprob(S) = -10.4291\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### N-gram LM Task 2: Language Generator (30')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your language generator should use the unsmoothed bigram language model to produce new\n",
        "sentences, probabilistically! Given a seed word, the language generation algorithm is:\n",
        "\n",
        "1. Find all bigrams that begin with the seed word - let’s call this set $B_{seed}$. Probabilistically select one of the bigrams in $B_{seed}$ with a likelihood proportional to its probability.\n",
        "For example, suppose “crazy” is the seed and exactly two bigrams begin with “crazy”:\n",
        "“crazy people” (frequency=10) and “crazy horse” (frequency=15).\n",
        "\n",
        "Consequently, $P (people | crazy) = \\frac{10}{25} = .40$ and $P (horse | crazy) = \\frac{15}{25} = .60$\n",
        "There should be a 40% chance that your program selects “crazy people” and a 60%\n",
        "chance that it selects “crazy horse”.\n",
        "An easy way to do this is to generate a random number x between [0,1]. Then establish\n",
        "ranges based on the bigram probabilities. For example, if 0 ≤ x ≤.40 then your pro-\n",
        "gram selects “crazy people”, but if.40 < x ≤ 1 then your program selects “crazy horse”.\n",
        "\n",
        "2. Let’s call the selected bigram $B^\\prime=w_0w_1$ (where $w_0$ is the seed). Generate $w_1$ as the next word in your new sentence.\n",
        "\n",
        "3. Return to Step 1 using $w_1$ as the new seed word to generate the next $w_2$ and continue.\n",
        "\n",
        "Your program should stop generating words when one of the following conditions exists:\n",
        "\n",
        "* your program generates one of three words: [. ? !]\n",
        "* your program generates 40 words (NOT including the original seed word)\n",
        "* $B_{seed}$ is empty (i.e., there are no bigrams that begin with the seed word)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IMPORTANT: For each seed word, your language generator should randomly generate 10\n",
        "sentences that begin with that word. Since each sentence is generated probabilistically, the\n",
        "sentences will (usually) be diﬀerent from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Output Specifications for Generation Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your program should print each seed word followed by a blank line and then the 10 sentences\n",
        "generated from that seed word. You should format your output like this:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Seed = <seed>\n",
        "\n",
        "Sentence 1: <sentence>\n",
        "Sentence 2: <sentence>\n",
        "Sentence 3: <sentence>\n",
        "Sentence 4: <sentence> \n",
        "Sentence 5: <sentence> \n",
        "Sentence 6: <sentence> \n",
        "Sentence 7: <sentence> \n",
        "Sentence 8: <sentence> \n",
        "Sentence 9: <sentence> \n",
        "Sentence 10: <sentence> \n",
        "\n",
        "Seed = <seed>\n",
        "\n",
        "Sentence 1: <sentence>\n",
        "Sentence 2: <sentence>\n",
        "Sentence 3: <sentence>\n",
        "Sentence 4: <sentence> \n",
        "Sentence 5: <sentence> \n",
        "Sentence 6: <sentence> \n",
        "Sentence 7: <sentence> \n",
        "Sentence 8: <sentence> \n",
        "Sentence 9: <sentence> \n",
        "Sentence 10: <sentence> \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Submission for Part 2\n",
        "\n",
        "The Part 2 does not depend on any the previous cells in this jupyter note book. Instead, you need to create your own python environment, and write your ngram.py from scratch. \n",
        "Since you need to output the log probabilities for the test file, and generate 10 sentences for each seed in the seeds file. \n",
        "Hence, please submit three things:\n",
        "\n",
        "1. The source code for your program. Be sure to include all files that are needed to run your program, include a conda environmental file!\n",
        "2. A README file that includes the following information:\n",
        "• how to run your code (suggest to use 3.10 python)\n",
        "• any known bugs, problems, or limitations of your program\n",
        "3. Submit two trace files: (1) ngram-prob.trace for the logprob outputs (2) ngram-gen.trace for the generated sentences for each seed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "G_u7lr90W2Ok",
        "3Vupo1DqW2Ol"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "cs5293-1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
